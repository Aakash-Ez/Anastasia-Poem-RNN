{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "punc pytorch-anastasia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXta1E-aEKBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = \"/content/drive/My Drive/dataset/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD3J6jd3GgiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Svr9dcH52-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import words\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from textblob import TextBlob\n",
        "def correct_word(word):\n",
        "  if word==\"\\n\":\n",
        "    return \"\\n\"\n",
        "  elif word==\"mot\":\n",
        "    return \"not\"\n",
        "  elif word==\"mo\":\n",
        "    return \"no\"\n",
        "  elif word ==\"j\":\n",
        "    return \"i\"\n",
        "  else:\n",
        "    return str(TextBlob(word).correct())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97d2Kz5aH6rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Anastasia(nn.Module):\n",
        "    def __init__(self, input_size,vocab_size):\n",
        "        super(Anastasia, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size,256,batch_first=True)\n",
        "        self.linear1 = nn.Linear(256,512)\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "        self.linear2 = nn.Linear(512,1024)\n",
        "        self.linear3 = nn.Linear(1024,1024)\n",
        "        self.dropout2 = nn.Dropout(0.75)\n",
        "        self.out = nn.Linear(1024,vocab_size)\n",
        "    def forward(self, input):\n",
        "        output, _ = self.lstm(input)\n",
        "        output = torch.tanh(output)\n",
        "        output = self.linear1(output)\n",
        "        output = F.leaky_relu(output)\n",
        "        output = self.dropout1(output)\n",
        "        output = self.linear2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.linear3(output)\n",
        "        output = torch.tanh(output)\n",
        "        output = self.dropout2(output)\n",
        "        output = self.out(output)\n",
        "        output = torch.reshape(output,(input.shape[0],vocab_len))\n",
        "        return output\n",
        "def to_categorical(y, num_classes):\n",
        "    return np.eye(num_classes, dtype='uint8')[y]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyAxExB4xUVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model,dataX,vocab_len,length=100,value=\"\"):\n",
        "    if len(value)<50:\n",
        "      start = np.random.randint(0, len(dataX)-1)\n",
        "      pattern = dataX[start]\n",
        "      head = X_son_head[start]\n",
        "      head = np.reshape(head,(1,50,1))\n",
        "    else:\n",
        "      val = value[:50]\n",
        "      pattern = [char_to_int[i] for i in val]\n",
        "      \"\"\"value = np.array(list(value))\n",
        "      pattern = np.reshape(value,(1,50,1))\"\"\"\n",
        "    word_out = \"\"\n",
        "    print(\"Given Input:\\n\",''.join([int_to_char[value] for value in pattern]))\n",
        "    count_nl = 0\n",
        "    print(\"Prediction:\")\n",
        "    for i in range(length):\n",
        "        x = np.reshape(pattern[-50:], (1, 1, 50))\n",
        "        x = x / float(vocab_len)\n",
        "        x = torch.tensor(x)\n",
        "        #x = np.array([np_utils.to_categorical(j,num_classes=vocab_len) for j in x])\n",
        "        prediction = F.softmax(model(x))\n",
        "        index = torch.argmax(prediction).item()\n",
        "        prediction = np.reshape(prediction.detach().numpy(),(vocab_len))\n",
        "        if int_to_char[index]==\"\\n\" and char_to_int['\\n'] in pattern[-40:]:\n",
        "          #sys.stdout.write(\"1\")\n",
        "          while int_to_char[index]==\"\\n\":\n",
        "            index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "        if ((pattern[-1]==char_to_int[\" \"] or pattern[-1]==char_to_int[\"\\n\"] or pattern[-1]==char_to_int[\"t\"]) and random.random()<0.25) or int_to_char[index]==\"-\":\n",
        "          #sys.stdout.write(\"2\")\n",
        "          index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "          while int_to_char[index]==\"\\n\" or int_to_char[index]==\" \" or int_to_char[index]==\"-\":\n",
        "            index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "        result = int_to_char[index]\n",
        "        if result == \" \" and char_to_int['\\n'] not in pattern[-40:len(pattern)]:\n",
        "          #sys.stdout.write(\"3\")\n",
        "          #print(pattern[10:len(pattern)],end='')\n",
        "          result = '\\n'\n",
        "          index = char_to_int['\\n']\n",
        "        if result == '\\n':\n",
        "          count_nl+=1\n",
        "          if count_nl>13:\n",
        "            break\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        #sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        word_out += result\n",
        "        word_out = word_out.replace(\"\\n\",\" \\n \")\n",
        "        wordslist = word_out.split()\n",
        "        #print(wordslist)\n",
        "        if len(wordslist)>2:\n",
        "          if wordslist[-1] == wordslist[-2] or (len(wordslist[-1])>9 and wordslist[-1] not in set(words.words())):\n",
        "            lastword = wordslist.pop(-1)\n",
        "            word_out = word_out[:-len(lastword)]\n",
        "            pattern = pattern[:-len(lastword)].copy()\n",
        "            i -=len(lastword)\n",
        "        #pattern = pattern[1:len(pattern)]\n",
        "    out = [int_to_char[i].replace(\"\\n\",\" \\n \") for i in pattern]\n",
        "    #print(out)\n",
        "    out = \"\".join(out)\n",
        "    out = [correct_word(i) for i in out.split(\" \")]\n",
        "    out =  \" \".join(out).replace(\"\\n \",\"\\n\")\n",
        "    print(out)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QRIjqDtsqZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Punctuation(string): \n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”“'''\n",
        "    for x in string.lower(): \n",
        "        if x in punctuations: \n",
        "            string = string.replace(x, \"\") \n",
        "    return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00u548yx0re7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2 = open(base_path+\"text.txt\",\"r\")\n",
        "data = file2.read().lower()\n",
        "sonnets_data = data.split(\"\\n\")\n",
        "print(sonnets_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CS48Qvt0teO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(data)))\n",
        "chars = sorted(chars)\n",
        "vocab_len = len(chars)\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(\"Vocab Length:\",vocab_len)\n",
        "print(int_to_char)\n",
        "print([(i,c) for i,c in enumerate(sonnets_data)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy55hqI90u34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_sonnets(sonnets_data,seq_len=50):\n",
        "  sonnets = []\n",
        "  seq_length = seq_len\n",
        "  dataX = []\n",
        "  dataX_Char = []\n",
        "  X_son_head = []\n",
        "  dataY = []\n",
        "  for i in range(2,len(sonnets_data),17):\n",
        "      string = \"\\n\".join(sonnets_data[i:i+14])\n",
        "      sonnets.append(string)\n",
        "  random.shuffle(sonnets)\n",
        "  print(sonnets)\n",
        "  for son in sonnets:\n",
        "      son_len = len(son)\n",
        "      son_head = [char_to_int[i] for i in son[0:seq_length]]\n",
        "      for j in range(0, son_len - seq_length, 1):\n",
        "        seq_in = son[j:j + seq_length]\n",
        "        seq_out = son[j + seq_length]\n",
        "        X_son_head.append(son_head)\n",
        "        dataX_Char.append([char for char in seq_in])\n",
        "        dataX.append([char_to_int[char] for char in seq_in])\n",
        "        dataY.append(char_to_int[seq_out])\n",
        "  pattern_len = len(dataX)\n",
        "  print(pattern_len)\n",
        "  X = np.reshape(dataX, (pattern_len, 1, seq_length))\n",
        "  X = torch.tensor(X / float(vocab_len)).double()\n",
        "  X_son_head = np.reshape(X_son_head, (pattern_len, 1, seq_length))\n",
        "  X_son_head = torch.tensor(X_son_head/ float(vocab_len)).double()\n",
        "  y = torch.tensor(dataY)\n",
        "  return dataX,dataY,X,y,X_son_head\n",
        "dataX,dataY,X,y,X_son_head = random_sonnets(sonnets_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-CV-xbD0xjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y.shape)\n",
        "print(X.shape)\n",
        "seq_length = 50\n",
        "size = y.shape[0]\n",
        "print(size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqjbEuYv4dpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_split = 0.7\n",
        "dataX,dataY,X,y,X_son_head = random_sonnets(sonnets_data)\n",
        "train_x = X[:int(train_split*size)]\n",
        "train_y = y[:int(train_split*size)]\n",
        "val_x = X[int(train_split*size):]\n",
        "val_y = y[int(train_split*size):]\n",
        "val_size = val_y.shape[0]\n",
        "filepath_load = base_path+\"pytorch_anastasia_attempt1_50_changeddropout_punc.pth\"\n",
        "filepath_save = base_path+\"pytorch_anastasia_attempt1_50_changeddropout_punc.pth\"\n",
        "current_val_loss = float('inf')\n",
        "current_val_acc = 0\n",
        "model = Anastasia(seq_length,vocab_len).double()\n",
        "print(model)\n",
        "model.load_state_dict(torch.load(filepath_load))\n",
        "epochs = 500\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batches = int(train_x.size()[0]/BATCH_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "lambdalr = lambda epoch: 0.1/(1+np.exp(epoch/80-2))+0.001\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambdalr])\n",
        "epoch_loss_list = []\n",
        "val_loss_list = []\n",
        "accuracy_list = []\n",
        "val_accuracy_list = []\n",
        "patience = 400\n",
        "checker = 0\n",
        "for epoch in range(epochs):\n",
        "  if epoch%10 == 0:\n",
        "    dataX,dataY,X,y,X_son_head = random_sonnets(sonnets_data)\n",
        "    train_x = X[:int(train_split*size)]\n",
        "    train_y = y[:int(train_split*size)]\n",
        "    val_x = X[int(train_split*size):]\n",
        "    val_y = y[int(train_split*size):]\n",
        "    val_size = val_y.shape[0]\n",
        "  if checker >= patience:\n",
        "    print(\"Val Loss didn't decrease in last 5 epochs\")\n",
        "    break\n",
        "  epoch_loss = 0\n",
        "  correct = 0\n",
        "  permutation = torch.randperm(train_x.size()[0])\n",
        "  for i in range(0,train_x.size()[0], BATCH_SIZE):\n",
        "    step = int(i/BATCH_SIZE)\n",
        "    optimizer.zero_grad()\n",
        "    indices = permutation[i:i+BATCH_SIZE]\n",
        "    batch_x, batch_y,batch_h = train_x[indices], train_y[indices],X_son_head[indices]\n",
        "    yhat = model(batch_x)\n",
        "    loss = criterion(yhat, batch_y.long())\n",
        "    epoch_loss += loss.item()\n",
        "    _,predicted=torch.max(yhat,1)\n",
        "    correct += (predicted==batch_y).sum().item()/BATCH_SIZE\n",
        "    if i>0:\n",
        "      print('\\rEpoch: '+str(epoch+1)+' Batch '+str(int(i))+' Loss: '+str(epoch_loss/i*BATCH_SIZE)+' Accuracy: '+str(correct/i*BATCH_SIZE)+' Steps left',batches-step,end=\"\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  val_yhat = model(val_x)\n",
        "  _,predicted_val=torch.max(val_yhat,1)\n",
        "  loss_val = criterion(val_yhat, val_y.long())\n",
        "  correct_val = (predicted_val==val_y).sum().item()/val_size\n",
        "  print(\"\\rEpoch\",epoch+1,\"completed. Loss\", epoch_loss/batches,\"Accuracy:\",correct/batches,\"Val Loss\", loss_val.item(),\"Val Accuracy:\",correct_val)\n",
        "  scheduler.step()\n",
        "  if loss_val < current_val_loss and correct_val > current_val_acc*0.995:\n",
        "    checker = 0\n",
        "    current_val_loss = loss_val\n",
        "    current_val_acc = correct_val\n",
        "    if loss_val < 4:\n",
        "      torch.save(model.state_dict(),filepath_save)\n",
        "      print(\"Val Loss decreased and the model state dict has been saved\")\n",
        "  else:\n",
        "    #loss_val.backward()\n",
        "    #optimizer.step()\n",
        "    if loss_val*0.98 > current_val_loss:\n",
        "      checker += 1\n",
        "  predict(model,dataX,vocab_len,100000,\"\")\n",
        "  if (epoch)%5 == 0:\n",
        "    val_loss_list.append(loss_val.item())\n",
        "    val_accuracy_list.append(correct_val)\n",
        "    epoch_loss_list.append(epoch_loss/batches)\n",
        "    accuracy_list.append(correct/batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTJQkyW7okxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_loss_list)\n",
        "plt.plot(val_loss_list)\n",
        "plt.show()\n",
        "plt.plot(val_accuracy_list)\n",
        "plt.plot(accuracy_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giuj-TNZu1Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_load = base_path+\"pytorch_anastasia_attempt1_50_changeddropout_punc.pth\"\n",
        "model = Anastasia(seq_length,vocab_len).double()\n",
        "model.load_state_dict(torch.load(filepath_load))\n",
        "predict_input(model,vocab_len,14,\"from the fairest creatures we desire increase,\\nwhile beauty may die in the\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcdqFO8NqhtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_input(model,vocab_len,newcount=14,value=\"\"):\n",
        "    val = value[:50]\n",
        "    pattern = [char_to_int[i] for i in val]\n",
        "    n_val = char_to_int['\\n']\n",
        "    word_out = \"\"\n",
        "    print(\"Given Input:\\n\",''.join([int_to_char[value] for value in pattern]))\n",
        "    count_nl = pattern.count(n_val)\n",
        "    print(\"Prediction:\")\n",
        "    i = 0\n",
        "    while count_nl!=newcount:\n",
        "        x = np.reshape(pattern[-50:], (1, 1, 50))\n",
        "        x = x / float(vocab_len)\n",
        "        x = torch.tensor(x)\n",
        "        #x = np.array([np_utils.to_categorical(j,num_classes=vocab_len) for j in x])\n",
        "        prediction = F.softmax(model(x))\n",
        "        index = torch.argmax(prediction).item()\n",
        "        prediction = np.reshape(prediction.detach().numpy(),(vocab_len))\n",
        "        if int_to_char[index]==\"\\n\" and char_to_int['\\n'] in pattern[-40:]:\n",
        "          #sys.stdout.write(\"1\")\n",
        "          while int_to_char[index]==\"\\n\":\n",
        "            index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "        if ((pattern[-1]==char_to_int[\" \"] or pattern[-1]==char_to_int[\"\\n\"] or pattern[-1]==char_to_int[\"t\"]) and random.random()<0.25) or int_to_char[index]==\"-\":\n",
        "          #sys.stdout.write(\"2\")\n",
        "          index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "          while int_to_char[index]==\"\\n\" or int_to_char[index]==\" \" or int_to_char[index]==\"-\":\n",
        "            index = np.random.choice(vocab_len, 1, p=prediction)[0]\n",
        "        result = int_to_char[index]\n",
        "        if result == \" \" and char_to_int['\\n'] not in pattern[-40:len(pattern)]:\n",
        "          #sys.stdout.write(\"3\")\n",
        "          #print(pattern[10:len(pattern)],end='')\n",
        "          result = '\\n'\n",
        "          index = char_to_int['\\n']\n",
        "        if result == '\\n':\n",
        "          count_nl+=1\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        #sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        word_out += result\n",
        "        word_out = word_out.replace(\"\\n\",\" \\n \")\n",
        "        wordslist = word_out.split()\n",
        "        #print(wordslist)\n",
        "        if len(wordslist)>2:\n",
        "          if wordslist[-1] == wordslist[-2] or len(wordslist[-1])>9:\n",
        "            lastword = wordslist.pop(-1)\n",
        "            word_out = word_out[:-len(lastword)]\n",
        "            pattern = pattern[:-len(lastword)].copy()\n",
        "            i -=len(lastword)\n",
        "        #pattern = pattern[1:len(pattern)]\n",
        "        i+=1\n",
        "    out = [int_to_char[i].replace(\"\\n\",\" \\n \") for i in pattern]\n",
        "    #print(out)\n",
        "    out = \"\".join(out)\n",
        "    out = [correct_word(i) for i in out.split(\" \")]\n",
        "    out =  \" \".join(out).replace(\"\\n \",\"\\n\")\n",
        "    print(out)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWt7--5Nsedf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}